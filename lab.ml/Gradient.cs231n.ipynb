{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Strategy #1: A first very bad idea solution: Random search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# assume X_train is the data where each column is an example (e.g. 3073 x 50,000)\n",
    "# assume Y_train are the labels (e.g. 1D array of 50,000)\n",
    "# assume the function L evaluates the loss function\n",
    "\n",
    "bestloss = float(\"inf\")\n",
    "for num in range(1000):\n",
    "    W = np.random.randn(10, 3073) * 0.0001 # generate random parameters\n",
    "    loss = L(X_train, Y_train, W) # get the loss over the entire training set\n",
    "    if loss < bestloss:\n",
    "        bestloss = loss\n",
    "        bestW = W\n",
    "    print ('in attempt %d the loss was %f, best %f' % (num , loss, bestlosss))\n",
    "\n",
    "# prints:\n",
    "# in attempt 0 the loss was 9.401632, best 9.401632\n",
    "# in attempt 1 the loss was 8.959668, best 8.959668\n",
    "# in attempt 2 the loss was 9.044034, best 8.959668\n",
    "# in attempt 3 the loss was 9.278948, best 8.959668\n",
    "# in attempt 4 the loss was 8.857370, best 8.857370\n",
    "# in attempt 5 the loss was 8.943151, best 8.857370\n",
    "# in attempt 6 the loss was 8.605604, best 8.605604\n",
    "# ... (trunctated: continues for 1000 lines)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Strategy #2: Random Local Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "W = np.random.randn(10,3073) * 0.001\n",
    "bestloss = float(\"inf\")\n",
    "for i in range(1000):\n",
    "    step_size = 0.0001\n",
    "    Wtry = W + np.random.randn(10,3073) * step_size\n",
    "    loss = L(Xtr_cols, Ytr, Wtry)\n",
    "    if loss < bestloss:\n",
    "        W = Wtry\n",
    "        bestloss = loss\n",
    "    print (\"iter %d loss is %f\" % (i, bestloss))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Strategy #3: Following the Gradient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def eval_numerical_gradient(f,x):\n",
    "    \"\"\"\n",
    "    a naive implementation of numerical gradient of f at x\n",
    "    - f should be a function that takes a single argument\n",
    "    - x is the point （numpy array）to evaluate the gradient at\n",
    "    \"\"\"\n",
    "    \n",
    "    fx = f(x) # evaluate function value at original point\n",
    "    grad = np.zeros(x.shape)\n",
    "    h = 0.00001\n",
    "    \n",
    "    # iterate over all indexes in x\n",
    "    it = np.nditer(x, flags=[multi_index], op_flags=['readwrite'])\n",
    "    while not it.finished:\n",
    "        \n",
    "        # evaluate function at x+h\n",
    "        ix = it.multi_index\n",
    "        old_value = x[ix]\n",
    "        x[ix] = old_value + h # increment by h\n",
    "        fxh = f(x) # evaluate f(x+h)\n",
    "        x[ix] = old_value # restore to previous value(very important)\n",
    "        \n",
    "        #compute the partial derivative\n",
    "        grad[ix] = (fxh - fx) / h # the slope\n",
    "        it.iternext() # step to next dimension\n",
    "    return grad\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Vanilla Gradient Descent\n",
    "\n",
    "while True:\n",
    "    weights_grad = evaluate_gradient(loss_fun, data, weights)\n",
    "    weights += -step_size * weights_grad # perform paramter update"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Vanilla Minibatch Gradient Descent\n",
    "\n",
    "while True:\n",
    "    data_batch = sample_training_data(data, 256) # sample 256 exaples\n",
    "    weights_grad = evaluate_gradient(loss_fun, data_batch, weights)\n",
    "    weights += -step_size * weights_grad # perform parameter update"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
