{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A Three Layer Example with Matrix Multiplication"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## FEEDFORWARD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### data type"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "$I \\in \\mathbb{R}^{3\\times1}$, $X_{hidden} \\in \\mathbb {R}^{3\\times1}$, $O_{hidden} \\in \\mathbb {R}^{3\\times1}$ , $X_{output} \\in \\mathbb {R}^{3\\times1}$ , $O_{output} \\in \\mathbb {R}^{3\\times1}$  \n",
    "$W_{input\\_hidden} \\in \\mathbb{R}^{3\\times3}$, $W_{hidden\\_output} \\in \\mathbb{R}^{3\\times3}$  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### process"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$X_{hidden} = W_{input\\_hidden} \\circ I$  \n",
    "$O_{hidden} = sigmoid(X_{hidden})$  \n",
    "$X_{output} = W_{hidden\\_output} \\circ O_{hidden}$  \n",
    "$O_{output} = sigmoid(X_{output})$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Backpropagating Errors From More Output Nodes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BACKFORWARD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** an example of 2x2 network **  \n",
    "\n",
    "$e_{hidden,1}$   \n",
    "= sum of split errors on links $w_{11}$ and $w_{12}$  \n",
    "= $e_{output,1}$ * $\\frac {w_{11}} {w_{11}+w_{21}}$ + $e_{output,2}$ * $\\frac {w_{12}} {w_{12}+w_{22}}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** $error_{hidden} = W^{T}_{hidden\\_output} \\circ error_{output}$ **"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Key points**:  \n",
    "* Backprogating the error can be expressed as a matrix multiplication  \n",
    "* This allows us to express it concisely, irrespective of network size, and also allows computer languages that understand matrix calculations to do the work more efficiently and quickly  \n",
    "* This means both feeding signals forward and error backpropagation can be made efficient using matrix calculations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How Do We Actually Update Weights ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "$$\\frac {\\partial E} {\\partial w_{jk}}$$  \n",
    "$$\\frac {\\partial} {\\partial w_{jk}}  \\sum_n (t_n - o_n)^2$$  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Key Points**:  \n",
    "* **Gradient descent** is a really good way of working out the minimum of a function, and it really works well when that function is so complex and difficult that we couldn't easily work it out mathematically using algebra.  \n",
    "* What's more, the method still works well when there are many parameters, something that causes other methods to fail or become impractical.  \n",
    "* This method is also **resilient** to imperfections in the data, we don't go wildly wrong if the function isn't quite perfectly described or we accidentally take a wrong step occasionally.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "wieght link in make your own neural network\n",
    "![][giraffe] \n",
    "[giraffe]:weight_link.png \"Picture of a Giraffe\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\frac {\\partial E} {\\partial w_{jk}} = \\frac {\\partial} {\\partial w_{jk}} (t_k - o_k)^2 $$  \n",
    "$$\\frac {\\partial E} {\\partial w_{jk}} = \\frac {\\partial E} {\\partial o_k} \\cdot \\frac {\\partial o_k} {\\partial w_{jk}}$$  \n",
    "$$\\frac {\\partial E} {\\partial w_{jk}} = -2(t_k-o_k) \\cdot \\frac {\\partial o_k} {\\partial w_{jk}}$$  \n",
    "$$\\frac {\\partial E} {\\partial w_{jk}} = -2(t_k - o_k) \\cdot \\frac {\\partial} {\\partial w_{jk}} sigmoid(\\sum_j w_{jk} \\cdot o_j)$$  \n",
    "$$\\frac {\\partial E} {\\partial w_{jk}} = -2(t_k - o_k) \\cdot sigmoid(\\sum_j w_{jk} \\cdot o_j) (1-sigmoid(\\sum_j w_{jk} \\cdot o_j)) \\frac {\\partial} {\\partial w_{jk}} (\\sum_j w_{jk} \\cdot o_j)$$  \n",
    "$$\\frac {\\partial E} {\\partial w_{jk}} = -2(t_k - o_k) \\cdot sigmoid(\\sum_j w_{jk} \\cdot o_j) (1-sigmoid(\\sum_j w_{jk} \\cdot o_j)) \\cdot  o_j$$  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\frac {\\partial E} {\\partial w_{ij}} = -(e_j) \\cdot sigmoid(\\sum_i w_{ij} \\cdot o_i) (1-sigmoid(\\sum_i w_{ij} \\cdot o_i)) \\cdot  o_i$$ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ new \\ w_{jk}  = old \\ w_{jk} - \\alpha \\cdot \\frac {\\partial E} {\\partial w_{jk}} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "bp_matrix in make your own neural network\n",
    "![][giraffe] \n",
    "[giraffe]:bp_matrix.png \"Picture of a Giraffe\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\Delta W_{jk} = \\alpha \\ * E_k\\ * \\ sigmoid(O_k) \\ * \\ (1-sigmoid(O_K)) \\cdot O_j^T$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Key Points** :  \n",
    "* A neural network's error is a function of the internal link weights.  \n",
    "* Improving a neural network means reducing this error - by changing those weights.  \n",
    "* Choosing the right weights directly is too difficult. An alternative approach is to iteratively improve the weights by descending the error funciton, taking small steps. Each step is taken in the direction of the greatest downward slope from your current position. This is called **gradient descent**.  \n",
    "* That error slope is possible to calculate using calculus that isn't too difficult.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> reference: **make your own neural network**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preparing Data "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Key Points**:  \n",
    "* Neural networks don't work well if the input, outp0ut and initial weight data is not prepared to match the network design and the actual problem being solved.   \n",
    "* A common problem is **saturation** - where large signals, sometimes driven by large weights, lead to signals that are at the very shallow slopes of the activation funciton. This reduces the ability to learn better weights.  \n",
    "* Another problem is zero value signals or weights. These also kill the ability to learn better weights.  \n",
    "* The internal link weights should be **random** and **small**, avoiding zero. Some will use more sophisticated rules, for example, reducing the size of these weights if there are more links into a node.  \n",
    "* **Inputs** should be scaled to be small, but not zero. A common range is 0.01 to 0.99, or -1.0 to +1.0, depending on which better matches the problem.  \n",
    "* **Outputs** should be within the range of what the activation function can produce. Values below 0 or above 1, inclusive, are impossible for the logistic sigmoid. Setting training targets outside the valid range will drive ever larger weights, leading to saturation. A good range is 0.01 to 0.99. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
